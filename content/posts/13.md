---
title: "Dynamic programming (4) Example" 
date: 2023-04-11
tags: ['dynamic programming']
author: "Me"
showToc: true
TocOpen: false
math: true
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: true
---

Consider a simple consumption-saving model, where action (a) is defined by the amount of savings each period, state (s) defined by the current stock, reward be the utility which depends on consumption (c=s-a). Suppose that state is updated where the output is drawn from a uniform distribution on {0, . . . , B}. Let the global upper bound of storage be M. 

## State space 

State space is \\n = M+B+1\\) dimension. 
$$ 
S = \lbrace{0, \cdots, M+B \rbrace}
$$ 

## The set of feasible actions at s 
The set of actions is \\(m = M+1\\) dimensions.
$$ 
A =  \lbrace{0, \cdots, M \rbrace}
$$ 

## Reward function 
$$
r(s, a) = u(c)=  u(s-a) = c^{\alpha} = (s-a)^{\alpha}
$$ 

## Transition probabilities
$$ 
Q(s, a, s^\prime) := 
\begin{cases}
    \frac{1}{B+1} & \text{if } a \leq s^\prime \leq a + B \newline
    0 & \text{otherwise}
\end{cases}
$$ 

## Defining instances

R is \\(n \times m \\) reward array, Q is \\(n \times m \times n\\) array.  Set R[s,a] = u(s-a) if \\(a\leq s\\) and negative infinity otherwise. 

## Julia codes
```
using BenchmarkTools, Plots, QuantEcon, Parameters
SimpleOG = @with_kw (B = 10, M = 5, α = 0.5, β = 0.9)

function transition_matrices(g)
    # unpack g to corresponding parameters
    (B, M, α, β) = g
    # define reward
    u(c) = c^α
    # define number of possible states (n) 
    n = B + M + 1
    # define number of possible actions (m)
    m = M + 1

    # set R, a reward matrix be n by m matrix, where 
    # row indicates state and column indicates actions
    R = zeros(n, m)
    # set Q, a transition matrix, a n by m by n matrix
    Q = zeros(n, m, n)

    # do the interation to produce all possible outcomes
    # for a predefined M
    for a in 0:M
        # for all rows of each column a and across a:a+B, apply same probabilities
        Q[:, a + 1, (a:(a + B)) .+ 1] .= 1 / (B + 1)
        # create reward function 
        # where it takes positive utility when consumption is positive, 
        # otherwise takes -inf
        for s in 0:(B + M)
            R[s + 1, a + 1] = (a≤s ? u(s - a) : -Inf)
        end
    end

    # return transition matrix Q and reward matrix R
    return (Q = Q, R = R)
end
```


## Reference

https://julia.quantecon.org/dynamic_programming/discrete_dp.html
 