---
title: "Dynamic programming (1)" 
date: 2023-04-06
tags: ['dynamic programming']
author: "Me"
showToc: true
TocOpen: false
math: true
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: true
---

In finite Markov Decision Process, we have three sets, a set of states, a set of actions, and a set of rewards.  Suppose that \\(R_t\\) and \\(S_t\\), which represent rewards and states at a given time \\(t\\) have well defined discrete probability distributions that only depends on previous action and state. 

$$ 
p(s^\prime, r | s, a) =\Pr(S_t=s^\prime,  R_t=r | S_{t-1}=s, A_{t-1}=a) 
$$ 

$$ 
\begin{aligned}
v_\pi(s) 
&= \mathbb{E}_\pi[G_t | S_t = s]
\end{aligned}
$$ 

$$ 
\begin{aligned}
v_\pi(s) 
&= \sum_a\pi(a|s) \sum_{s^\prime, r}p(s^\prime, r | s, a)[r + \gamma v_\pi(s^\prime)], \text{ for all } s \in S
\end{aligned}
$$ 

## Reference 

Richard S. Sutton and Andrew G. Barto, "Reinforcement learning: An introduction", Second Edition, MIT Press
