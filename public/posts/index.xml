<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Boyoon&#39;s blog</title>
    <link>https://boyoon-c.github.io/posts/</link>
    <description>Recent content in Posts on Boyoon&#39;s blog</description>
    <image>
      <title>Boyoon&#39;s blog</title>
      <url>https://boyoon-c.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://boyoon-c.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 11 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://boyoon-c.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Dynamic programming (4) Example</title>
      <link>https://boyoon-c.github.io/posts/13/</link>
      <pubDate>Tue, 11 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://boyoon-c.github.io/posts/13/</guid>
      <description>Consider a simple consumption-saving model, where action (a) is defined by the amount of savings each period, state (s) defined by the current stock, reward be the utility which depends on consumption (c=s-a). Suppose that state is updated where the output is drawn from a uniform distribution on {0, . . . , B}. Let the global upper bound of storage be M.
State space State space is \n = M+B+1\) dimension.</description>
    </item>
    
    <item>
      <title>Dynamic programming (3) Discrete DPs</title>
      <link>https://boyoon-c.github.io/posts/12/</link>
      <pubDate>Mon, 10 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://boyoon-c.github.io/posts/12/</guid>
      <description>Let \(s_t\) denotes the state variable, \(a_t\) denotes the action, \(\beta\) denotes a discount factor. Note that \(r(a_t, s_t)\) can be interpreted as a current reward that is a function of the current action and current state.</description>
    </item>
    
    <item>
      <title>Dynamic programming (2) Rewards</title>
      <link>https://boyoon-c.github.io/posts/11/</link>
      <pubDate>Sun, 09 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://boyoon-c.github.io/posts/11/</guid>
      <description>Rewards The goal of the agent is to maximize the cumulative sum of the rewards of the long-run. The rewards could be arbitrarily chosen number that summarizes how one wants the agent to behave under specific state, action, and subsequent state.
The rewards function, formally represented by \(R(s)\) or \(R(s,a)\), or \(R(s, a, s^\prime)\) can depend on current state, the subsequent state as well as the action taken by the agent taken in the current state.</description>
    </item>
    
    <item>
      <title>Some random thoughts</title>
      <link>https://boyoon-c.github.io/posts/10/</link>
      <pubDate>Fri, 07 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://boyoon-c.github.io/posts/10/</guid>
      <description>Some random thoughts</description>
    </item>
    
    <item>
      <title>Dynamic programming (1) MDP</title>
      <link>https://boyoon-c.github.io/posts/09/</link>
      <pubDate>Thu, 06 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://boyoon-c.github.io/posts/09/</guid>
      <description>The agent and the environment In finite Markov Decision Process (MDP), we have three sets, a set of states, a set of actions, and a set of rewards.
The learner or decision maker is called agent and the outside system that the agent interacts with is called environment. Everyperiod, the agent takes actions and correspondingly the environment reacts to produce new states to the agent.
Each period, the environment presents \(S_t\) from a set \( S\).</description>
    </item>
    
    <item>
      <title>Map and broadcast</title>
      <link>https://boyoon-c.github.io/posts/08/</link>
      <pubDate>Wed, 05 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://boyoon-c.github.io/posts/08/</guid>
      <description>Map map(f, c...) applies function f to each element of collection c that enter as the the second argument of the map function.
Below two lines of codes produce same output. This means that map also acts as a for loop.
map(x-&amp;gt;x+2, 1:10) [ x + 2 for x=1:10] Output:
10-element Vector{Int64}: 3 4 5 6 7 8 9 10 11 12 The function that enters the first argument of map could take multiple arguments; it could be more than one argument.</description>
    </item>
    
    <item>
      <title>Comprehension</title>
      <link>https://boyoon-c.github.io/posts/07/</link>
      <pubDate>Tue, 04 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://boyoon-c.github.io/posts/07/</guid>
      <description>Comprehensions works similar to for loop except that it can be expressed into a single line.
For example, below will iterate the process of summing three variables that takes different integer in a range of 1 and 3. This will produce 27 different cases each of which will be stored in 3x3x3 array.
[ i + j + k for i=1:3, j=1:3, k=1:3 ] Output:
3×3×3 Array{Int64, 3}: [:, :, 1] = 3 4 5 4 5 6 5 6 7 [:, :, 2] = 4 5 6 5 6 7 6 7 8 [:, :, 3] = 5 6 7 6 7 8 7 8 9 </description>
    </item>
    
    <item>
      <title>Optimization Using Julia</title>
      <link>https://boyoon-c.github.io/posts/04/</link>
      <pubDate>Mon, 03 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://boyoon-c.github.io/posts/04/</guid>
      <description>This post is about optimization using Julia</description>
    </item>
    
    <item>
      <title>What job might not be replaceable by AI? </title>
      <link>https://boyoon-c.github.io/posts/05/</link>
      <pubDate>Sun, 02 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://boyoon-c.github.io/posts/05/</guid>
      <description>This post explores jobs that might be hard to be replaced by AI</description>
    </item>
    
    <item>
      <title>Modules in Julia</title>
      <link>https://boyoon-c.github.io/posts/06/</link>
      <pubDate>Sat, 01 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://boyoon-c.github.io/posts/06/</guid>
      <description>Desc</description>
    </item>
    
    <item>
      <title>For loop, RData, filter, and wsample in Julia</title>
      <link>https://boyoon-c.github.io/posts/03/</link>
      <pubDate>Fri, 31 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://boyoon-c.github.io/posts/03/</guid>
      <description>This post is using for loop, importing R object into Julia, using filter to DataFrame-type-object, using wsample to draw values from known probability distribution</description>
    </item>
    
    <item>
      <title>Basics of Julia</title>
      <link>https://boyoon-c.github.io/posts/02/</link>
      <pubDate>Thu, 30 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://boyoon-c.github.io/posts/02/</guid>
      <description>This post is about basics of Julia. This is an exercise before running the simulation for my job market paper using Julia</description>
    </item>
    
    <item>
      <title>Perfect Prediction?</title>
      <link>https://boyoon-c.github.io/posts/01/</link>
      <pubDate>Mon, 30 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://boyoon-c.github.io/posts/01/</guid>
      <description>Some random thoughts</description>
    </item>
    
  </channel>
</rss>
